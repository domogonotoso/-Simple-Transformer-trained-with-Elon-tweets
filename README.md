##  Simple Transformer trained with Elon tweets


elon-transformer/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ elon_tweets.csv         # Raw Elon tweet data collected at Kaggle
â”‚   â””â”€â”€ processed.txt           # Cleaned and preprocessed data used for training
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ transformer.py          # Definition of the Transformer model architecture
â”‚   â””â”€â”€ tokenizer.py            # Tokenizer class or wrapper (custom or HuggingFace-based)
â”‚
â”œâ”€â”€ trainer/
â”‚   â””â”€â”€ train.py                # Training loop and optimization logic
â”‚
â”œâ”€â”€ generate/
â”‚   â””â”€â”€ generate.py             # Inference script to generate text from a prompt
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ preprocess.py           # Functions for cleaning and preparing text data
â”‚   â””â”€â”€ plot_loss.py            # Utility to plot training loss or perplexity
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml             # Configuration file for model hyperparameters
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ best_model.pth          # Checkpoint of the trained model
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ samples.txt             # Output samples generated by the model
â”‚   â””â”€â”€ loss_plot.png           # Training loss curve visualization
â”‚
â”œâ”€â”€ main.py                     # Entry point: trains or generates text depending on argument
â”œâ”€â”€ README.md                   # Project overview, setup instructions, and sample outputs
â”œâ”€â”€ requirements.txt            # Python package dependencies
â””â”€â”€ .gitignore                  # Files and directories to ignore in Git



Use pretrained GPT2 Tokenizer, which is Byte-Pair-Encoding. Not use word unit text encoder, instead, we use it at a binary scale.

You can download elon_musk_tweets.csv from kaggle
https://www.kaggle.com/datasets/gpreda/elon-musk-tweets?resource=download
use trained tokenizer from huggingface
attention map
block size

---
GELU


* Models like **GPT-2, GPT-3, BERT, RoBERTa, and T5**â€”essentially **almost all Transformer-based architectures**â€”use **GELU** activations.

* The reasons are:

  * When dealing with **subtle semantic differences** in natural language, **GELU provides smoother and more continuous gradients** than ReLU, which helps with training stability.
  * Especially during the **early stages of training**, ReLU can suffer from the **"dead neuron" problem**, whereas GELU avoids that with its softer activation curve.

---
token
encoding(positive encoding)
attention
skip connection
decoding

## Implementation

```bash
python -m utils.preprocess

python 

```

![train&val](results/loss_plot.png)
[Epoch 70] Train Loss: 0.0684 | Val Loss: 2.1454
âœ… Final model saved.

It trained well.



ğŸ’¬ Enter a prompt: tesla will   

ğŸ“ Generated text:
tesla willbuterin balajis zengjiajuneth had to do a doubletake this is the first nonfake vitalik tweet ive seen inmber seems spacex tesladarve evafoxu muk evafiecha a gailalfaratx teslaownersebay mydogect bocachiciatorbay blueskyklic tesla douglewinenergy dimazeniuk evafoxuini latekeniukinkatto



```python
    # Remove mentions, hashtags, HTML entities
    text = re.sub(r'@\w+', '', text)           
    text = re.sub(r'#\w+', '', text)           
    text = re.sub(r'&\w+;', '', text)          
```

```bash
    python -m utils.preprocess
    python -m trainer.train
    python -m generate.generate
```

```text
Refined dataset has more less text tokens than before ones
Change block size 32 to 16
  
```

ğŸ’¬ Enter a prompt: Tesla 


ğŸ“ Generated text:

Tesla problem is that the passiveindex funds outsource voting to shareholder services companies which are cnn soros interesting account he would be identical emails to like i thought incarceration control civilization if nonkraine how many paper should get at truth which are them is all at all at that are at w i talked to run the public freefall follow or without consciousness solve weak to see anything down to runkraine tons maybe someone asked for at twitter or both scams last night on earth soon as it is all at the

Not word text token, BPE(Byte pair encoding). Because this we can see something that literally not like real words.
So, let's put text what model learned from processed.txt and give it longer one.



Data : population of mars is still zero people
ğŸ’¬ Enter a prompt: population of mars
ğŸ“ Generated text:

population of mars community notes is perfect because he would not be an actual useful orbit with a vital i know its only security did their investment to be even though to drama their career a strong prominent get spacexa should harder to keep things is very risky about raising to hit where were extreme there is impossible and easy of aliens twitter becomesller to keep sign as possible to wish it be who incorrectly the at surfacing the left wing thrott bad at the left wing stuff as the probability the environment those who they cant protect heart



Dataset : for improved quality of sleep raise head of your bed by about or cm and dont eat hours before bedtime
ğŸ’¬ Enter a prompt: for improved quality of sleep raise
ğŸ“ Generated text:

for improved quality of sleep raise head of your bed by about or cm and dont eat hours earlier tracking service on twitter and dont impersonation which is neither was set things control access its good at w were some roadster half its own which dont i believe any mistakes on the left are extremely m its only once who died it has its own into that kinetic energy even though nothing be said so sigh me at least a massive escalation by stalker within the difa has sta within micro except people and if at twitter spoke and i

It seemed better and write exactly what they learned. But it is not enough. It doesn't to stop talking and not know where to stop. 
So I decided to do two ways. First, improve dataset to add punct at eos(end of sentence). Second, do knowledge distillation