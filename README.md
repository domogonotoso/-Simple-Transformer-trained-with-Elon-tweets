##  Simple Transformer trained with Elon tweets


elon-transformer/
├── data/
│   ├── elon_tweets.csv         # Raw Elon tweet data collected at Kaggle
│   └── processed.txt           # Cleaned and preprocessed data used for training
│
├── model/
│   ├── transformer.py          # Definition of the Transformer model architecture
│   └── tokenizer.py            # Tokenizer class or wrapper (custom or HuggingFace-based)
│
├── trainer/
│   └── train.py                # Training loop and optimization logic
│
├── generate/
│   └── generate.py             # Inference script to generate text from a prompt
│
├── utils/
│   ├── preprocess.py           # Functions for cleaning and preparing text data
│   └── plot_loss.py            # Utility to plot training loss or perplexity
│
├── config/
│   └── config.yaml             # Configuration file for model hyperparameters
│
├── checkpoints/
│   └── best_model.pth          # Checkpoint of the trained model
│
├── results/
│   ├── samples.txt             # Output samples generated by the model
│   └── loss_plot.png           # Training loss curve visualization
│
├── main.py                     # Entry point: trains or generates text depending on argument
├── README.md                   # Project overview, setup instructions, and sample outputs
├── requirements.txt            # Python package dependencies
└── .gitignore                  # Files and directories to ignore in Git



Use pretrained GPT2 Tokenizer, which is Byte-Pair-Encoding. Not use word unit text encoder, instead, we use it at a binary scale.

You can download elon_musk_tweets.csv from kaggle
https://www.kaggle.com/datasets/gpreda/elon-musk-tweets?resource=download
use trained tokenizer from huggingface
attention map
block size

---
GELU


* Models like **GPT-2, GPT-3, BERT, RoBERTa, and T5**—essentially **almost all Transformer-based architectures**—use **GELU** activations.

* The reasons are:

  * When dealing with **subtle semantic differences** in natural language, **GELU provides smoother and more continuous gradients** than ReLU, which helps with training stability.
  * Especially during the **early stages of training**, ReLU can suffer from the **"dead neuron" problem**, whereas GELU avoids that with its softer activation curve.

---

encoding(positive encoding)
attention
skip connection
decoding

## Implementation

```bash
python -m utils.preprocess

python 

```

![train&val](results/loss_plot.png)
[Epoch 70] Train Loss: 0.0684 | Val Loss: 2.1454
✅ Final model saved.

It trained well.